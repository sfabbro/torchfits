[workspace]
preview = ["pixi-build"]
channels = ["conda-forge"]
platforms = ["linux-64", "osx-arm64", "osx-64"]

[package]
name = "torchfits"
version = "0.1.0"
description = "High-performance FITS I/O for PyTorch"
authors = ["Seb Fabbro <sebfabbro@gmail.com>"]

[package.build]
backend = { name = "pixi-build-cmake", version = "*"}

# Build dependencies - tools needed to build the package
# Note: cmake, ninja, and cxx-compiler are automatically provided by pixi-build-cmake
[package.build-dependencies]
pybind11 = ">=2.11.0"
scikit-build-core = ">=0.10"
cmake = ">=3.21"
ninja = ">=1.10"

# Host dependencies - libraries that will be linked into the package
[package.host-dependencies]
python = ">=3.11"
pytorch = ">=2.0"
cfitsio = ">=4"
wcslib = ">=7.11"

# Run dependencies - packages needed at runtime
[package.run-dependencies]
pytorch = ">=2.0"
pytorch-frame = ">=0.2.5"
psutil = ">=5.0"  # For system resource monitoring

# Workspace dependencies - development environment
[dependencies]
torchfits = { path = "." }
pytest = ">=8.4.1,<9"
pytest-cov = ">=6.2.1,<7"
ruff = ">=0.12.9,<0.13"
mypy = ">=1.17.1,<2"
pip = ">=25.2,<26"
astropy = ">=7.1.0,<8"
fitsio = ">=1.2.6,<2"
matplotlib = ">=3.10.5,<4"
seaborn = ">=0.13.2,<0.14"
pandas = ">=2.3.1,<3"
psutil = ">=7.0.0,<8"
scikit-build-core = ">=0.11.5,<0.12"
cmake = ">=4.1.0,<5"
ninja = ">=1.13.1,<2"

[tasks]
# Development tasks
dev = "pip install -e . --no-build-isolation"
build = "pixi build"
test = "pytest tests/"
lint = "ruff check . && ruff format ."
verify = "python verify_setup.py"

# Core benchmarks (essential)
bench = "python benchmarks/benchmark_all.py --output-dir benchmark_results"
bench-basic = "python benchmarks/benchmark_basic.py"
bench-core = "python benchmarks/benchmark_core.py"
bench-ml = "python benchmarks/benchmark_ml.py"
bench-table = "python benchmarks/benchmark_table.py"

# Comprehensive benchmarks (renamed from exhaustive)
bench-all = "python benchmarks/benchmark_all.py"
bench-all-keep = "python benchmarks/benchmark_all.py --no-cleanup"

# Comprehensive benchmark runner
bench-all-auto = "python benchmarks/run_all_benchmarks.py --no-exhaustive"
bench-all-runner = "python benchmarks/run_all_benchmarks.py"
bench-all-force = "python benchmarks/run_all_benchmarks.py --exhaustive"
bench-transforms = "python benchmarks/benchmark_transforms.py"
bench-buffer = "python benchmarks/benchmark_buffer.py"
bench-cache = "python benchmarks/benchmark_cache.py"
bench-focused = "python benchmarks/benchmark_transforms.py && python benchmarks/benchmark_buffer.py && python benchmarks/benchmark_cache.py"

# Pytest-based benchmarking
bench-pytest = "pytest benchmarks/ --benchmark-only"

# Development tools
notebook = "jupyter lab"

# Test phases for development workflow
test-phase1 = "python -c 'import sys; sys.path.insert(0, \"src\"); import torchfits; print(\"Phase 1 COMPLETE: All core features implemented\")'"  
test-comprehensive = "python examples/example_comprehensive_ml_pipeline.py"
test-transforms = "python -m pytest tests/test_transforms.py -v"
test-dataloader = "python -m pytest tests/test_dataloader.py -v"
test-buffer = "python -m pytest tests/test_buffer.py -v"

# Test feature with comparison libraries
[feature.test.dependencies]
astropy = ">=5.0"
fitsio = ">=1.0"

# Benchmark feature with additional dependencies
[feature.bench.dependencies]
astropy = ">=5.0"
fitsio = ">=1.0"
matplotlib = "*"
seaborn = "*"
pandas = "*"
psutil = "*"
pytest-benchmark = "*"

# Environment definitions
[environments]
default = { solve-group = "default" }
test = { features = ["test"], solve-group = "default" }
bench = { features = ["bench"], solve-group = "default" }
