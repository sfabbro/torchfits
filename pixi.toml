[workspace]
preview = ["pixi-build"]
channels = ["conda-forge"]
platforms = ["osx-arm64"]

[package]
name = "torchfits"
version = "0.2.0"
description = "High-performance FITS I/O for PyTorch"
authors = ["Seb Fabbro <sebfabbro@gmail.com>"]

[package.build]
backend = { name = "pixi-build-cmake", version = "*"}

# Build dependencies - tools needed to build the package
# Note: cmake, ninja, and cxx-compiler are automatically provided by pixi-build-cmake
[package.build-dependencies]
nanobind = ">=2.10.2"
scikit-build-core = ">=0.11.6"
cmake = ">=3.21"
ninja = ">=1.10"
python = ">=3.11"
pytorch = ">=2.0"
cfitsio = ">=4"
wcslib = ">=7.11"

# Host dependencies - libraries that will be linked into the package
[package.host-dependencies]
python = ">=3.11"
pytorch = ">=2.0"
cfitsio = ">=4"
wcslib = ">=7.11"
nanobind = ">=2.10.2"


# Run dependencies - packages needed at runtime
[package.run-dependencies]
pytorch = ">=2.0"

psutil = ">=5.0"  # For system resource monitoring
cfitsio = ">=4"
wcslib = ">=7.11"

# Workspace dependencies - development environment
[dependencies]
# torchfits = { path = "." }  # Don't add self-dependency - causes circular dependency
pytest = ">=8.0"
pytest-cov = ">=5.0"
ruff = ">=0.3"
mypy = ">=1.9"
pip = ">=24.0"
astropy = ">=5.0"
fitsio = ">=1.2"
matplotlib = ">=3.8"
seaborn = ">=0.13"
pandas = ">=2.2"
psutil = ">=5.9"
scikit-build-core = ">=0.11.6"
cmake = ">=3.21"
ninja = ">=1.10"
nanobind = ">=2.10.2"
polars = ">=0.20"
python = ">=3.11"
pytorch = ">=2.0"

cfitsio = ">=4.4"
wcslib = ">=8.0"
tabulate = ">=0.9.0"
twine = ">=5.0"

[tasks]
# Development tasks
dev = "pip install -e . --no-build-isolation"
build = "pixi build"
test = "pytest tests/"
lint = "ruff check . && ruff format ."
verify = "python verify_setup.py"
release-manual = "twine upload release_0.2.1/*"

# Core benchmarks (essential)
bench = "python benchmarks/benchmark_all.py --output-dir benchmark_results --include-tables && python benchmarks/benchmark_table.py && python benchmarks/benchmark_arrow_tables.py --output benchmark_results/table_arrow_results.csv && python benchmarks/benchmark_arrow_tables_diverse.py --output benchmark_results/table_arrow_diverse_results.csv"
bench-fast = "python benchmarks/benchmark_fast.py --output benchmark_results/fast_results.csv && python benchmarks/benchmark_table.py --quick && python benchmarks/benchmark_arrow_tables.py --rows 100000 --iterations 3 --warmup 1 --batch-size 50000 --output benchmark_results/table_arrow_results.csv && python benchmarks/benchmark_arrow_tables_diverse.py --rows 50000 --iterations 2 --warmup 1 --batch-size 50000 --output benchmark_results/table_arrow_diverse_results.csv"
bench-fast-stable = "python benchmarks/benchmark_fast.py --output benchmark_results/fast_results.csv --torch-threads 1 --compressed-repeats 3 && python benchmarks/benchmark_table.py --quick && python benchmarks/benchmark_arrow_tables.py --rows 100000 --iterations 3 --warmup 1 --batch-size 50000 --output benchmark_results/table_arrow_results.csv && python benchmarks/benchmark_arrow_tables_diverse.py --rows 50000 --iterations 2 --warmup 1 --batch-size 50000 --output benchmark_results/table_arrow_diverse_results.csv"
bench-core = "python benchmarks/benchmark_fast.py"
bench-table = "python benchmarks/benchmark_table.py"
bench-table-arrow = "python benchmarks/benchmark_arrow_tables.py"
bench-table-arrow-diverse = "python benchmarks/benchmark_arrow_tables_diverse.py"

# Comprehensive benchmarks (renamed from exhaustive)
bench-all = "python benchmarks/benchmark_all.py --profile user --include-tables"
bench-all-keep = "python benchmarks/benchmark_all.py --profile user --include-tables --no-cleanup"

# Comprehensive benchmark runner
bench-all-runner = "python benchmarks/benchmark_all.py --profile user --include-tables"
bench-all-force = "python benchmarks/benchmark_all.py --profile lab --include-tables"
bench-transforms = "python benchmarks/benchmark_transforms.py"
bench-buffer = "python benchmarks/benchmark_buffer.py"
bench-cache = "python benchmarks/benchmark_cache.py"
bench-focused = "python benchmarks/benchmark_transforms.py && python benchmarks/benchmark_buffer.py && python benchmarks/benchmark_cache.py"

# Pytest-based benchmarking
bench-pytest = "pytest benchmarks/ --benchmark-only"

# Development tools
notebook = "jupyter lab"

# Test phases for development workflow
test-phase1 = "python -c 'import sys; sys.path.insert(0, \"src\"); import torchfits; print(\"Phase 1 COMPLETE: All core features implemented\")'"
test-comprehensive = "python examples/example_ml_pipeline.py"
test-transforms = "python -m pytest tests/test_transforms.py -v"
test-dataloader = "python -m pytest tests/test_dataloader.py -v"
test-buffer = "python -m pytest tests/test_buffer.py -v"

# Test feature with comparison libraries
[feature.test.dependencies]
astropy = ">=5.0"
fitsio = ">=1.2"

# Benchmark feature with additional dependencies
[feature.bench.dependencies]
astropy = ">=5.0"
fitsio = ">=1.2"
matplotlib = ">=3.8"
seaborn = ">=0.13"
pandas = ">=2.2"
psutil = ">=5.9"
pytest-benchmark = ">=4.0"

# Environment definitions
[environments]
default = { solve-group = "default" }
test = { features = ["test"], solve-group = "default" }
bench = { features = ["bench"], solve-group = "default" }
